# Scalability Considerations

To ensure the recommendation system can handle millions of daily prediction requests at Zomato's scale, we prioritized low-latency architectural decisions:

## 1. Latency Optimization (< 300ms SLA)
- **Measured Inference:** Our offline evaluation pipeline clocked an average end-to-end inference latency of ~40ms per request. This easily satisfies the stringent < 300ms SLA requirement for real-time cart prediction.
- **Precomputed Item Embeddings:** To guarantee this speed despite using transformer models for embeddings, **all catalog item embeddings are precomputed offline**. At runtime, we only embed and mean-pool the active cart items.
- **Two-Stage Funneling:** By using the Vector Embedding graph as a "first pass" retrieval mechanism to fetch candidates out of a massive global catalog linearly, we prevent the heavy LightGBM Machine Learning ranker from running excessive evaluations on irrelevant items.

## 2. Model Size and Overhead
- **`all-MiniLM-L6-v2`:** We specifically chose this sentence-transformer model because it is incredibly lightweight ($~90$ MB footprint) while perfectly adequate for semantic mapping of food terminology. Memory loading issues on containerized worker nodes are virtually non-existent.
- **LightGBM:** This framework is notoriously efficient, storing decision trees in integer-mapped leaf structures rather than bloated floating-point memory, minimizing cold-start spin-up times for the API endpoint.

## 3. Pre-Deployment Benchmarking Strategy
- **Load Testing Tool:** Before migrating to production, the Python API (`inference.py`) will be wrapped in a FastAPI endpoint and load-tested using **Locust**, an open-source load-testing framework.
- **Simulated Load:** We will spin up distributed Locust worker nodes to simulate 10,000 Concurrent Users (CCUs) hitting the recommendation endpoint during a mock "Friday Evening Dinner Rush."
- **Success Criteria:** The benchmark must prove that the p99 response time remains under the strict 300ms SLA, and that horizontal Node.js pod auto-scaling accurately triggers when CPU utilization breaches 75%.

## 4. Data Freshness
- The training pipeline script (`generate_synthetic_data.py`) generates a temporal split of behavior over a rolling 90-day window. The LightGBM feature weights are designed to be efficiently retrained on weekly cron-jobs to consistently learn new shifting culinary trends over time.
